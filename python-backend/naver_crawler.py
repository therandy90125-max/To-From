#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ëŸ¬
KOSPI/KOSDAQ ì£¼ì‹ ëª©ë¡ ìˆ˜ì§‘ ë° JSON íŒŒì¼ ìƒì„±
"""
import requests
from bs4 import BeautifulSoup
import json
import time
import sys
from pathlib import Path

# Windows ì½˜ì†” UTF-8 ì¸ì½”ë”© ì„¤ì •
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except:
        pass

def crawl_kospi(max_pages=5):
    """KOSPI ì£¼ì‹ ëª©ë¡ í¬ë¡¤ë§ (ì—¬ëŸ¬ í˜ì´ì§€)"""
    all_stocks = []
    
    for page in range(1, max_pages + 1):
        url = f"https://finance.naver.com/sise/sise_market_sum.naver?sosok=0&page={page}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        # ë„¤ì´ë²„ëŠ” euc-kr ë˜ëŠ” utf-8 ì‚¬ìš© (í˜ì´ì§€ì— ë”°ë¼ ë‹¤ë¦„)
        if 'charset' in response.headers.get('content-type', ''):
            if 'euc-kr' in response.headers.get('content-type', '').lower():
                response.encoding = 'euc-kr'
            else:
                response.encoding = 'utf-8'
        else:
            # ê¸°ë³¸ê°’: euc-kr ì‹œë„
            try:
                response.encoding = 'euc-kr'
            except:
                response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ëª¨ë“  í…Œì´ë¸” ì°¾ê¸°
        tables = soup.find_all('table')
        table = None
        for t in tables:
            # type_1, type_2, item_list ë“± ë‹¤ì–‘í•œ í´ë˜ìŠ¤ ì‹œë„
            if t.get('class') and ('type' in str(t.get('class')) or 'item' in str(t.get('class'))):
                table = t
                break
        
        if not table and tables:
            table = tables[0]  # ì²« ë²ˆì§¸ í…Œì´ë¸” ì‚¬ìš©
        
            if not table:
                print(f"[WARNING] KOSPI í˜ì´ì§€ {page} í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            page_stocks = []
        for row in table.find_all('tr')[1:]:  # í—¤ë” ì œì™¸
            cols = row.find_all('td')
            if len(cols) < 2:
                continue
            
            # ì²« ë²ˆì§¸ ì—´: í‹°ì»¤ (ë§í¬ì—ì„œ ì¶”ì¶œ ì‹œë„)
            ticker = None
            ticker_link = cols[0].find('a')
            if ticker_link:
                href = ticker_link.get('href', '')
                # hrefì—ì„œ í‹°ì»¤ ì¶”ì¶œ: /item/main.naver?code=005930
                if 'code=' in href:
                    ticker = href.split('code=')[1].split('&')[0].strip()
                else:
                    ticker = cols[0].text.strip()
            else:
                ticker = cols[0].text.strip()
            
            # ë‘ ë²ˆì§¸ ì—´: íšŒì‚¬ëª…
            name_ko = cols[1].text.strip() if len(cols) > 1 else ""
            
            # ë¹ˆ ê°’ ì œì™¸
            if not ticker or not name_ko:
                continue
            
            # í‹°ì»¤ê°€ 6ìë¦¬ ìˆ«ìì¸ì§€ í™•ì¸
            if ticker.isdigit() and len(ticker) == 6:
                # ì¤‘ë³µ ì²´í¬
                if not any(s['ticker'] == ticker for s in all_stocks):
                    page_stocks.append({
                        "ticker": ticker,
                        "name_ko": name_ko,
                        "name_en": "",  # ë‚˜ì¤‘ì— ë§¤í•‘
                        "market": "KOSPI",
                        "sector": ""  # ë‚˜ì¤‘ì— ì¶”ê°€
                    })
        
            if not page_stocks:
                # ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                break
            
            all_stocks.extend(page_stocks)
            print(f"  í˜ì´ì§€ {page}: {len(page_stocks)}ê°œ ìˆ˜ì§‘")
            time.sleep(0.5)  # ìš”ì²­ ê°„ê²©
        
        except Exception as e:
            print(f"[ERROR] KOSPI í˜ì´ì§€ {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            break
    
    return all_stocks

def crawl_kosdaq(max_pages=10):
    """KOSDAQ ì£¼ì‹ ëª©ë¡ í¬ë¡¤ë§ (ì—¬ëŸ¬ í˜ì´ì§€)"""
    all_stocks = []
    
    for page in range(1, max_pages + 1):
        url = f"https://finance.naver.com/sise/sise_market_sum.naver?sosok=1&page={page}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            # ë„¤ì´ë²„ëŠ” euc-kr ë˜ëŠ” utf-8 ì‚¬ìš© (í˜ì´ì§€ì— ë”°ë¼ ë‹¤ë¦„)
            if 'charset' in response.headers.get('content-type', ''):
                if 'euc-kr' in response.headers.get('content-type', '').lower():
                    response.encoding = 'euc-kr'
                else:
                    response.encoding = 'utf-8'
            else:
                # ê¸°ë³¸ê°’: euc-kr ì‹œë„
                try:
                    response.encoding = 'euc-kr'
                except:
                    response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ëª¨ë“  í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table')
            table = None
            for t in tables:
                # type_1, type_2, item_list ë“± ë‹¤ì–‘í•œ í´ë˜ìŠ¤ ì‹œë„
                if t.get('class') and ('type' in str(t.get('class')) or 'item' in str(t.get('class'))):
                    table = t
                    break
            
            if not table and tables:
                table = tables[0]  # ì²« ë²ˆì§¸ í…Œì´ë¸” ì‚¬ìš©
            
            if not table:
                print(f"[WARNING] KOSDAQ í˜ì´ì§€ {page} í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            page_stocks = []
            for row in table.find_all('tr')[1:]:  # í—¤ë” ì œì™¸
                cols = row.find_all('td')
                if len(cols) < 2:
                    continue
                
                # ì²« ë²ˆì§¸ ì—´: í‹°ì»¤ (ë§í¬ì—ì„œ ì¶”ì¶œ ì‹œë„)
                ticker = None
                ticker_link = cols[0].find('a')
                if ticker_link:
                    href = ticker_link.get('href', '')
                    # hrefì—ì„œ í‹°ì»¤ ì¶”ì¶œ: /item/main.naver?code=005930
                    if 'code=' in href:
                        ticker = href.split('code=')[1].split('&')[0].strip()
                    else:
                        ticker = cols[0].text.strip()
                else:
                    ticker = cols[0].text.strip()
                
                # ë‘ ë²ˆì§¸ ì—´: íšŒì‚¬ëª…
                name_ko = cols[1].text.strip() if len(cols) > 1 else ""
                
                # ë¹ˆ ê°’ ì œì™¸
                if not ticker or not name_ko:
                    continue
                
                # í‹°ì»¤ê°€ 6ìë¦¬ ìˆ«ìì¸ì§€ í™•ì¸
                if ticker.isdigit() and len(ticker) == 6:
                    # ì¤‘ë³µ ì²´í¬
                    if not any(s['ticker'] == ticker for s in all_stocks):
                        page_stocks.append({
                            "ticker": ticker,
                            "name_ko": name_ko,
                            "name_en": "",  # ë‚˜ì¤‘ì— ë§¤í•‘
                            "market": "KOSDAQ",
                            "sector": ""  # ë‚˜ì¤‘ì— ì¶”ê°€
                        })
            
            if not page_stocks:
                # ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                break
            
            all_stocks.extend(page_stocks)
            print(f"  í˜ì´ì§€ {page}: {len(page_stocks)}ê°œ ìˆ˜ì§‘")
            time.sleep(0.5)  # ìš”ì²­ ê°„ê²©
        
        except Exception as e:
            print(f"[ERROR] KOSDAQ í˜ì´ì§€ {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            break
    
    return all_stocks

def load_existing_mapping():
    """ê¸°ì¡´ JSON íŒŒì¼ì—ì„œ ì˜ë¬¸ëª… ë§¤í•‘ ë¡œë“œ"""
    data_dir = Path(__file__).parent / 'data'
    json_file = data_dir / 'korean_stocks.json'
    
    if not json_file.exists():
        return {}
    
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
        
        # {ticker: (name_en, name_ko)} í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        mapping = {}
        for stock in existing_data:
            ticker = stock.get('ticker', '')
            if ticker:
                mapping[ticker] = {
                    'name_en': stock.get('name_en', ''),
                    'name_ko': stock.get('name_ko', ''),
                    'sector': stock.get('sector', '')
                }
        
        return mapping
    except Exception as e:
        print(f"ê¸°ì¡´ ë§¤í•‘ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return {}

def enrich_with_existing_data(crawled_stocks, existing_mapping):
    """í¬ë¡¤ë§ëœ ë°ì´í„°ì— ê¸°ì¡´ ì˜ë¬¸ëª…/ì„¹í„° ì •ë³´ ì¶”ê°€"""
    enriched = []
    
    for stock in crawled_stocks:
        ticker = stock['ticker']
        
        if ticker in existing_mapping:
            # ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©
            stock['name_en'] = existing_mapping[ticker]['name_en']
            stock['sector'] = existing_mapping[ticker]['sector']
            # name_koëŠ” í¬ë¡¤ë§ëœ ìµœì‹  ë°ì´í„° ì‚¬ìš©
        else:
            # ì˜ë¬¸ëª…ì´ ì—†ìœ¼ë©´ í•œê¸€ëª… ì‚¬ìš©
            stock['name_en'] = stock['name_ko']
        
        enriched.append(stock)
    
    return enriched

def save_to_json(stocks, filename='korean_stocks_crawled.json'):
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    data_dir = Path(__file__).parent / 'data'
    data_dir.mkdir(exist_ok=True)
    
    json_file = data_dir / filename
    
    # í‹°ì»¤ ê¸°ì¤€ ì •ë ¬
    stocks_sorted = sorted(stocks, key=lambda x: x['ticker'])
    
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(stocks_sorted, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {json_file}")
    print(f"   ì´ {len(stocks_sorted)}ê°œ ì£¼ì‹")

def merge_with_existing():
    """í¬ë¡¤ë§ëœ ë°ì´í„°ì™€ ê¸°ì¡´ ë°ì´í„° ë³‘í•©"""
    data_dir = Path(__file__).parent / 'data'
    crawled_file = data_dir / 'korean_stocks_crawled.json'
    existing_file = data_dir / 'korean_stocks.json'
    
    # í¬ë¡¤ë§ëœ ë°ì´í„° ë¡œë“œ
    if not crawled_file.exists():
        print("[ERROR] í¬ë¡¤ë§ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    with open(crawled_file, 'r', encoding='utf-8') as f:
        crawled = json.load(f)
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    existing = []
    if existing_file.exists():
        with open(existing_file, 'r', encoding='utf-8') as f:
            existing = json.load(f)
    
    # ë³‘í•©: í¬ë¡¤ë§ëœ ë°ì´í„° ìš°ì„ , ê¸°ì¡´ ë°ì´í„°ì˜ ì˜ë¬¸ëª…/ì„¹í„° ì •ë³´ ë³´ì¡´
    existing_dict = {s['ticker']: s for s in existing}
    merged = []
    merged_tickers = set()
    
    # í¬ë¡¤ë§ëœ ë°ì´í„° ì¶”ê°€ (ì˜ë¬¸ëª…/ì„¹í„° ì •ë³´ ë³´ì¡´)
    for stock in crawled:
        ticker = stock['ticker']
        if ticker in existing_dict:
            # ê¸°ì¡´ ë°ì´í„°ì˜ ì˜ë¬¸ëª…/ì„¹í„° ì‚¬ìš©
            stock['name_en'] = existing_dict[ticker].get('name_en', stock['name_ko'])
            stock['sector'] = existing_dict[ticker].get('sector', '')
        merged.append(stock)
        merged_tickers.add(ticker)
    
    # ê¸°ì¡´ ë°ì´í„° ì¤‘ í¬ë¡¤ë§ë˜ì§€ ì•Šì€ í•­ëª© ì¶”ê°€
    for stock in existing:
        if stock['ticker'] not in merged_tickers:
            merged.append(stock)
    
    # ìµœì¢… ì €ì¥
    save_to_json(merged, 'korean_stocks.json')
    
    # í†µê³„ ì¶œë ¥
    kospi_count = sum(1 for s in merged if s['market'] == 'KOSPI')
    kosdaq_count = sum(1 for s in merged if s['market'] == 'KOSDAQ')
    
    print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
    print(f"   - ì´ ì£¼ì‹ ìˆ˜: {len(merged)}ê°œ")
    print(f"   - KOSPI: {kospi_count}ê°œ")
    print(f"   - KOSDAQ: {kosdaq_count}ê°œ")

if __name__ == '__main__':
    print("=" * 60)
    print("ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 60)
    
    # KOSPI í¬ë¡¤ë§
    print("\n[1/2] KOSPI í¬ë¡¤ë§ ì¤‘...")
    kospi_stocks = crawl_kospi()
    print(f"[OK] KOSPI: {len(kospi_stocks)}ê°œ ìˆ˜ì§‘")
    
    # KOSDAQ í¬ë¡¤ë§
    print("\n[2/2] KOSDAQ í¬ë¡¤ë§ ì¤‘...")
    time.sleep(1)  # ìš”ì²­ ê°„ê²©
    kosdaq_stocks = crawl_kosdaq()
    print(f"[OK] KOSDAQ: {len(kosdaq_stocks)}ê°œ ìˆ˜ì§‘")
    
    # ê¸°ì¡´ ë§¤í•‘ ë¡œë“œ
    print("\nê¸°ì¡´ ì˜ë¬¸ëª… ë§¤í•‘ ë¡œë“œ ì¤‘...")
    existing_mapping = load_existing_mapping()
    print(f"[OK] {len(existing_mapping)}ê°œ ë§¤í•‘ ë¡œë“œ")
    
    # ë°ì´í„° ë³‘í•©
    all_stocks = kospi_stocks + kosdaq_stocks
    enriched_stocks = enrich_with_existing_data(all_stocks, existing_mapping)
    
    # í¬ë¡¤ë§ëœ ë°ì´í„° ì €ì¥
    print("\ní¬ë¡¤ë§ëœ ë°ì´í„° ì €ì¥ ì¤‘...")
    save_to_json(enriched_stocks, 'korean_stocks_crawled.json')
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
    print("\nê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© ì¤‘...")
    merge_with_existing()
    
    print("\n" + "=" * 60)
    print("í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 60)

